{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download file from MTA\n",
    "#Input into a PD Dataframe (e.g. correctly label columns)\n",
    "#Helper tables: Define MTA station Mapping\n",
    "#reformat PD into a usable DF - Unit ID, Date-time, station name, entries, exits\n",
    "#sort on unit-id (will sort by time too). remove duplicates. [possibly find a better method]\n",
    "\n",
    "#- Which station has the most number of units? ... Penn Station (unique turnstile units for the Week of Feb 2nd)\n",
    "#- What is the total number of entries & exits across the subway system for February 1, 2013? ...neet to calculate net entries and exits [2hrs]\n",
    "#- Let’s define the busy-ness as sum of entry & exit count. What station was the busiest on February 1, 2013? What turnstile was the busiest on that date? ... add busyness column. filter for only feb 1. group by station [30 mins]\n",
    "#- What stations have seen the most usage growth/decline in 2013? [NEEDS clarification - since daily variance is significant] ...need to download whole dataset for 2012 and 2013. (speed up calculation)...sum up net transactions for the year [3hrs]\n",
    "#- What dates are the least busy? Could you identify days on which stations were not operating at full capacity or closed entirely? ... [2hrs]\n",
    "#- Bonus:  What hour is the busiest for station CANAL ST in Q1 2013?\n",
    "\n",
    "#volume by day by station, by turnstile\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Questions\n",
    "#1) What kind of assumptions can I make about the data (e.g. can I assume counts are accurate?)\n",
    "#2) What does \"door open logon lgf man...\" mean?\n",
    "#3) Does question 1 refer to number of units on Feb 1?\n",
    "#4) I'm making the assumption about day-cutoffs. credit it based on the day the 4 hour cutof ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def addTimeStamp(vec):\n",
    "    x = str(vec[0])\n",
    "    y = str(vec[1])\n",
    "    try:\n",
    "        return datetime.strptime(x + \" \" + y, \"%m-%d-%y %H:%M:%S\" ).strftime(\"%y-%m-%d %H:%M:%S\")\n",
    "    except:\n",
    "        return \"NULL\"\n",
    "\n",
    "def read_MTA_CSV(filename):\n",
    "    raw_MTA_data = pd.read_csv(filename)\n",
    "    raw_MTA_data.columns= [\"C/A\",\"UNIT\",\"SCP\",\"DATE1\",\"TIME1\",\"DESC1\",\"ENTRIES1\",\"EXITS1\",\"DATE2\",\"TIME2\",\"DESC2\",\"ENTRIES2\",\"EXITS2\",\"DATE3\",\n",
    "    \"TIME3\",\"DESC3\",\"ENTRIES3\",\"EXITS3\",\"DATE4\",\"TIME4\",\"DESC4\",\"ENTRIES4\",\"EXITS4\",\"DATE5\",\"TIME5\",\"DESC5\",\"ENTRIES5\",\"EXITS5\",\"DATE6\",\n",
    "    \"TIME6\",\"DESC6\",\"ENTRIES6\",\"EXITS6\",\"DATE7\",\"TIME7\",\"DESC7\",\"ENTRIES7\",\"EXITS7\",\"DATE8\",\"TIME8\",\"DESC8\",\"ENTRIES8\",\"EXITS8\"]\n",
    "    #print(raw_MTA_data.head())\n",
    "\n",
    "    #Loop through raw dataset. for each row, make an entry in the MTA turnstile data set.\n",
    "    raw_MTA_data_tmp = raw_MTA_data.copy(deep=True)\n",
    "    raw_MTA_data_tmp[\"Unit_ID\"] = raw_MTA_data_tmp[\"C/A\"] + raw_MTA_data_tmp[\"UNIT\"] + raw_MTA_data_tmp[\"SCP\"]\n",
    "    raw_MTA_data_tmp = raw_MTA_data_tmp.drop([\"C/A\", \"SCP\"],axis=1)\n",
    "\n",
    "    for i in range(1,8+1):\n",
    "        raw_MTA_data_tmp[\"Datetime_\" + str(i)] = raw_MTA_data_tmp[[\"DATE\" + str(i), \"TIME\" + str(i)]].apply(addTimeStamp,axis=1)\n",
    "\n",
    "    #raw_MTA_data_tmp[\"Date_time_1\"] = raw_MTA_data_tmp.apply(lambda x: datetime.strptime(x[\"DATE1\"] + \" \" + x[\"TIME1\"], \"%m-%d-%y %H:%M:%S\" ).strftime(\"%y-%m-%d %H:%M:%S\"),axis=1) \n",
    "    raw_MTA_data_tmp = raw_MTA_data_tmp.drop([\"DATE1\", \"TIME1\", \"DATE2\", \"TIME2\", \"DATE3\", \"TIME3\", \"DATE4\", \"TIME4\", \"DATE5\", \"TIME5\", \"DATE6\", \"TIME6\", \"DATE7\", \"TIME7\", \"DATE8\", \"TIME8\"],axis=1)\n",
    "    #print(raw_MTA_data_tmp.head())\n",
    "    return raw_MTA_data_tmp\n",
    "\n",
    "raw_MTA_data_tmp = read_MTA_CSV(\"~/Projects/NYC_MTA_DATA_ANALYSIS/data/turnstile_130202.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: 'Which station has the most number of units?'\n",
      "count    100\n",
      "Name: 34 ST-PENN STA, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Question 1: 'Which station has the most number of units?'\")\n",
    "\n",
    "#add station name & count number of turnstiles per station\n",
    "def addStationName(filename, raw_MTA_data_tmp):\n",
    "    if 'Station' not in raw_MTA_data_tmp.columns:\n",
    "        remote_station_mapping = pd.read_excel(filename)\n",
    "        remote_station_mapping = remote_station_mapping.drop_duplicates(subset=[\"Remote\"]) #assumption only keep the first occuring station name (e.g. 59th st vs. Lexington ave)\n",
    "        remote_station_mapping[\"UNIT\"] = remote_station_mapping[\"Remote\"]\n",
    "\n",
    "        raw_MTA_data_tmp = raw_MTA_data_tmp.merge(remote_station_mapping[[\"UNIT\", \"Station\"]], on=[\"UNIT\"])\n",
    "    return raw_MTA_data_tmp\n",
    "\n",
    "raw_MTA_data_tmp = addStationName(\"../data/Remote-Booth-Station.xls\", raw_MTA_data_tmp)\n",
    "MTA_turnstiles_unique = raw_MTA_data_tmp.drop_duplicates(subset=[\"Unit_ID\"]).copy(deep=True)\n",
    "stationWithMostTerminals = MTA_turnstiles_unique.groupby(by=\"Station\").agg([\"count\"])[\"UNIT\"].sort_values(by=\"count\",).iloc[-1,:]\n",
    "\n",
    "print(stationWithMostTerminals)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Station               Unit           Datetime  \\\n",
      "0              59 ST   A002R05102-00-00  13-01-27 11:00:00   \n",
      "1              59 ST   A002R05102-00-00  13-01-28 19:00:00   \n",
      "2              59 ST   A002R05102-00-00  13-01-30 03:00:00   \n",
      "3              59 ST   A002R05102-00-00  13-01-31 11:00:00   \n",
      "4              59 ST   A002R05102-00-00  13-02-01 15:00:00   \n",
      "...              ...                ...                ...   \n",
      "30856  RIT-ROOSEVELT  TRAM2R46900-05-01  13-01-29 20:00:00   \n",
      "30857  RIT-ROOSEVELT  TRAM2R46900-05-01  13-01-30 07:48:11   \n",
      "30858  RIT-ROOSEVELT  TRAM2R46900-05-01  13-01-31 08:00:00   \n",
      "30859  RIT-ROOSEVELT  TRAM2R46900-05-01  13-02-01 16:00:00   \n",
      "30860  RIT-ROOSEVELT  TRAM2R46900-05-01               NULL   \n",
      "\n",
      "       Cumulative entries  Cumulative exits     Desc  \n",
      "0               3967826.0         1367578.0  REGULAR  \n",
      "1               3969685.0         1368236.0  REGULAR  \n",
      "2               3971424.0         1368820.0  REGULAR  \n",
      "3               3973086.0         1369749.0  REGULAR  \n",
      "4               3974913.0         1370326.0  REGULAR  \n",
      "...                   ...               ...      ...  \n",
      "30856              5554.0              99.0  REGULAR  \n",
      "30857              5554.0             100.0  REGULAR  \n",
      "30858              5554.0             100.0  REGULAR  \n",
      "30859              5554.0             100.0  REGULAR  \n",
      "30860                 NaN               NaN      NaN  \n",
      "\n",
      "[246888 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#convert dataframe so that each row has only 1 4-hour block of data\n",
    "\n",
    "def expandRows(raw_MTA_data_tmp):\n",
    "    raw_MTA_data_tmp_x8 = pd.DataFrame()\n",
    "    for i in range(1,8+1):\n",
    "        raw_MTA_data_tmp_copy = raw_MTA_data_tmp.copy(deep=True)\n",
    "        raw_MTA_data_tmp_copy[\"Unit\"] = raw_MTA_data_tmp_copy[\"Unit_ID\"]\n",
    "        raw_MTA_data_tmp_copy[\"Datetime\"] = raw_MTA_data_tmp_copy[\"Datetime_\" + str(i)]\n",
    "        raw_MTA_data_tmp_copy[\"Station\"] = raw_MTA_data_tmp_copy[\"Station\"]\n",
    "        raw_MTA_data_tmp_copy[\"Cumulative entries\"] = raw_MTA_data_tmp_copy[\"ENTRIES\" + str(i)]\n",
    "        raw_MTA_data_tmp_copy[\"Cumulative exits\"] = raw_MTA_data_tmp_copy[\"EXITS\" + str(i)]\n",
    "        raw_MTA_data_tmp_copy[\"Desc\"] = raw_MTA_data_tmp_copy[\"DESC\" + str(i)]\n",
    "        raw_MTA_data_tmp_x8 = pd.concat([raw_MTA_data_tmp_x8,raw_MTA_data_tmp_copy])\n",
    "\n",
    "    return raw_MTA_data_tmp_x8[raw_MTA_data_tmp_x8.columns[-6:]].copy(deep=True)\n",
    "\n",
    "raw_MTA_data_tmp = expandRows(raw_MTA_data_tmp)\n",
    "\n",
    "print(raw_MTA_data_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate net entries and exits for each turnstile-date and each turnstile\n",
    "\n",
    "def getNetEntriesExits(MTA_turnstiles):\n",
    "    #sort & make ID the index...\n",
    "    MTA_turnstiles[\"ID\"] = MTA_turnstiles[\"Unit\"] + \" \" + MTA_turnstiles[\"Datetime\"] \n",
    "    MTA_turnstiles_sorted = MTA_turnstiles.sort_values(by=[\"ID\"])\n",
    "    #print(len(MTA_turnstiles_sorted))\n",
    "    MTA_turnstiles_sorted = MTA_turnstiles_sorted.drop_duplicates(subset=[\"ID\"])\n",
    "    #print(len(MTA_turnstiles_sorted))\n",
    "\n",
    "    #calculate net entries / exits for the day\n",
    "    MTA_turnstiles_grouped = MTA_turnstiles_sorted.groupby([\"Unit\"])\n",
    "    MTA_turnstiles_sorted[\"Net entries\"] = MTA_turnstiles_grouped[\"Cumulative entries\"].transform(pd.Series.diff)\n",
    "    MTA_turnstiles_sorted[\"Net exits\"] = MTA_turnstiles_grouped[\"Cumulative exits\"].transform(pd.Series.diff)\n",
    "\n",
    "    #data cleaning (deal with edge cases like negatives, NaN, etc. )\n",
    "    # (MTA_turnstiles_sorted[\"Net entries\"].astype(\"float64\").describe(percentiles=[.1,.25,.5,.75,.9]))\n",
    "    # print(MTA_turnstiles_sorted[\"Net exits\"].describe())\n",
    "    # print('Number of negative entries: %d' %len(MTA_turnstiles_sorted.loc[MTA_turnstiles_sorted['Net entries'] < 0])) #caused by system resets\n",
    "    # print('Number of negative exits: %d' %len(MTA_turnstiles_sorted.loc[MTA_turnstiles_sorted['Net exits'] < 0])) #caused by system resets\n",
    "    # print('Number of NaN entries: %d' %len(MTA_turnstiles_sorted.loc[MTA_turnstiles_sorted['Net entries'].isnull()])) #caused by net zero difference\n",
    "    # print('Number of NaN exits: %d' %len(MTA_turnstiles_sorted.loc[MTA_turnstiles_sorted['Net exits'].isnull()])) #caused by net zero difference\n",
    "\n",
    "    #set outliers to zero\n",
    "    MTA_turnstiles_sorted.loc[MTA_turnstiles_sorted[\"Net entries\"] < 0] = 0\n",
    "    MTA_turnstiles_sorted.loc[MTA_turnstiles_sorted[\"Net exits\"] < 0] = 0\n",
    "    MTA_turnstiles_sorted.loc[MTA_turnstiles_sorted[\"Net entries\"] > 10000] = 0 #assumption: max error threshold\n",
    "    MTA_turnstiles_sorted.loc[MTA_turnstiles_sorted[\"Net exits\"] > 10000] = 0 #assumption: max error threshold\n",
    "    MTA_turnstiles_sorted[\"Net entries\"] = MTA_turnstiles_sorted[\"Net entries\"].fillna(0)\n",
    "    MTA_turnstiles_sorted[\"Net exits\"] = MTA_turnstiles_sorted[\"Net exits\"].fillna(0)\n",
    "\n",
    "    # print('Number of negative entries: %d' %len(MTA_turnstiles_sorted.loc[MTA_turnstiles_sorted['Net entries'] < 0])) #caused by system resets\n",
    "    # print('Number of negative exits: %d' %len(MTA_turnstiles_sorted.loc[MTA_turnstiles_sorted['Net exits'] < 0])) #caused by system resets\n",
    "    # print('Number of NaN entries: %d' %len(MTA_turnstiles_sorted.loc[MTA_turnstiles_sorted['Net entries'].isnull()])) #caused by net zero difference\n",
    "    # print('Number of NaN exits: %d' %len(MTA_turnstiles_sorted.loc[MTA_turnstiles_sorted['Net exits'].isnull()])) #caused by net zero difference\n",
    "\n",
    "    MTA_turnstiles_sorted[\"Date\"] = pd.to_datetime(MTA_turnstiles_sorted[\"Datetime\"], errors=\"coerce\", yearfirst = True).dt.strftime('%y-%m-%d')\n",
    "    MTA_turnstiles_by_day = MTA_turnstiles_sorted.groupby(by=[\"Unit\", \"Date\"]).agg({\"Net entries\": [\"sum\"]})\n",
    "    MTA_turnstiles_by_day = MTA_turnstiles_by_day.reset_index()\n",
    "    MTA_turnstiles_by_day.columns = MTA_turnstiles_by_day.columns.get_level_values(0)\n",
    "    return MTA_turnstiles_sorted, MTA_turnstiles_by_day\n",
    "\n",
    "MTA_turnstiles = raw_MTA_data_tmp.copy(deep=True)\n",
    "MTA_turnstiles, MTA_turnstiles_by_day  = getNetEntriesExits(MTA_turnstiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 2: What is the total number of entries & exits across the subway system for February 1, 2013?\n",
      "5818588.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Question 2: What is the total number of entries & exits across the subway system for February 1, 2013?\")\n",
    "print(MTA_turnstiles_by_day.loc[MTA_turnstiles_by_day[\"Date\"] == \"13-02-01\"].sum()[\"Net entries\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 3: Let’s define the busy-ness as sum of entry & exit count. What station was the busiest on February 1, 2013? What turnstile was the busiest on that date?\n",
      "The most busy station is 34 ST-PENN STA and it had 348286 entries/exits\n"
     ]
    }
   ],
   "source": [
    "print(\"Question 3: Let’s define the busy-ness as sum of entry & exit count. What station was the busiest on February 1, 2013? What turnstile was the busiest on that date?\")\n",
    "MTA_turnstiles[\"Busy-ness\"] = MTA_turnstiles[\"Net entries\"] + MTA_turnstiles[\"Net exits\"] \n",
    "\n",
    "MTA_stations_by_day = MTA_turnstiles.groupby(by=[\"Station\", \"Date\"]).agg({\"Busy-ness\": [\"sum\"]})\n",
    "MTA_stations_by_day = MTA_stations_by_day.reset_index()\n",
    "MTA_stations_by_day.columns = MTA_stations_by_day.columns.get_level_values(0)\n",
    "MTA_stations_by_day = MTA_stations_by_day.loc[MTA_stations_by_day[\"Date\"] == \"13-02-01\"].sort_values(by=\"Busy-ness\")\n",
    "\n",
    "busiestStationName = MTA_stations_by_day.iloc[-1][\"Station\"]\n",
    "busiestStationTransactions = int(MTA_stations_by_day.iloc[-1][\"Busy-ness\"])\n",
    "\n",
    "print(\"The most busy station is {0} and it had {1} entries/exits\".format(busiestStationName, busiestStationTransactions)) #likely  times square, grand central,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Question 4: What stations have seen the most usage growth/decline in 2013?\") \n",
    "# Build this data frame for 2012 and 2013.\n",
    "# rows = calendar days for that year (365)\n",
    "# columns = busyness for each station, \"day of week\" (~300)\n",
    "\n",
    "#1. To create each dataframe, write a method which iterates over all weeks of that year. \n",
    "#   1a. For each week, return a dataframe with rows = days, columns = busyness for each station\n",
    "#   1b. append to the original dataframe\n",
    "#2. groupby day to sum up total busyness (~300 x 52 rows --> ~300x 1 rows)\n",
    "#3. Create a row with summation of all dates\n",
    "#4. Transpose 2012 and 2013 stations x day. merge into one dataset\n",
    "#5. Calculate usage growth / decline by station\n",
    "\n",
    "def readMTA_CSV(daysXStations, dateStr):\n",
    "    #turnstile x (8 hrs of data)\n",
    "    raw_data = pd.read_csv(\"~/Projects/NYC_MTA_DATA_ANALYSIS/data/turnstile_130202.txt\")\n",
    "\n",
    "    #turnstile x (1 hrs of data)\n",
    "    \n",
    "    #turnstile x (1 day of data)\n",
    "\n",
    "    #station x (1 day of data)\n",
    "\n",
    "    #station x (365 days of data)\n",
    "    return daysXStations\n",
    "\n",
    "daysXStations2013 = pd.DataFrame()\n",
    "daysXStations2013 = readMTA_CSV(daysXStations2013,\"2013-2-13\")\n",
    "\n",
    "print(daysXStations2013)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Question 5: What dates are the least busy? Could you identify days on which stations were not operating at full capacity or closed entirely?\")\n",
    "#Goal 1: Find dates which are least busy\"\n",
    "#1. Using 2013 dataset from above, create a new column which is the sum of busyness for all stations for a single day.\n",
    "\n",
    "#Goal 2: Find days on which stations aren't at fully capacity or closed entirely\n",
    "# 1. Use 2013 dataset from above. classify each day as a weekday or weekend.\n",
    "# 2. Add two new rows = weekend capacity and weekday capacity. this will be equal to the max value for each column\n",
    "# 3. Then, for each station, iterate through all weekdays and weekends of 2013. return all dates where there is < 50% capacity or 0% capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MTA_turnstiles[\"Index column\"] = \n",
    "MTA_turnstiles[\"Column 1\"]\n",
    "MTA_turnstiles[\"Column 2\"] = "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e92a65bea0caed0c1b2e444ffa4813b1725cbc7086e0d7c70a637107a7975c88"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('twosigma': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}